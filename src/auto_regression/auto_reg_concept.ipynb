{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: Autoregressive Text Generation with GPT-2\n",
    "\n",
    "This code demonstrates **step-by-step text generation** using the **GPT-2 model** from the Hugging Face `transformers` library.\n",
    "\n",
    "The model generates text in an *autoregressive manner*, predicting **one token at a time** and appending it to the input.\n",
    "\n",
    "```python\n",
    "# Example Input Prompt:\n",
    "\"What is the capital of France?\"\n",
    "```\n",
    "\n",
    "### Process Overview:\n",
    "1. **Convert the input text into tokens** using the GPT-2 tokenizer.\n",
    "2. **Feed the tokens into the GPT-2 model** to predict the next token.\n",
    "3. **Append the predicted token** to the current sequence.\n",
    "4. **Repeat until** the model predicts the **end-of-sequence (EOS)** token or reaches a specified token limit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Prompt: What is the capital of France?\n",
      "\n",
      "Step-by-Step Generation:\n",
      "Step 1: What is the capital of France?\n",
      "\n",
      "size of input ndarray is torch.Size([1, 7, 50257])\n",
      "Step 2: What is the capital of France?\n",
      "\n",
      "\n",
      "size of input ndarray is torch.Size([1, 8, 50257])\n",
      "Step 3: What is the capital of France?\n",
      "\n",
      "The\n",
      "size of input ndarray is torch.Size([1, 9, 50257])\n",
      "Step 4: What is the capital of France?\n",
      "\n",
      "The capital\n",
      "size of input ndarray is torch.Size([1, 10, 50257])\n",
      "Step 5: What is the capital of France?\n",
      "\n",
      "The capital of\n",
      "size of input ndarray is torch.Size([1, 11, 50257])\n",
      "Step 6: What is the capital of France?\n",
      "\n",
      "The capital of France\n",
      "size of input ndarray is torch.Size([1, 12, 50257])\n",
      "Step 7: What is the capital of France?\n",
      "\n",
      "The capital of France is\n",
      "size of input ndarray is torch.Size([1, 13, 50257])\n",
      "Step 8: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris\n",
      "size of input ndarray is torch.Size([1, 14, 50257])\n",
      "Step 9: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "size of input ndarray is torch.Size([1, 15, 50257])\n",
      "Step 10: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "size of input ndarray is torch.Size([1, 16, 50257])\n",
      "\n",
      "Final Output:\n",
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")           # Load the pretrained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_prompt = \"What is the capital of France?\"             # Set the input prompt\n",
    "\n",
    "input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\")         # Tokenize the input prompt\n",
    "print(\"Input Prompt:\", input_prompt)\n",
    "\n",
    "#### Autoregressive generation (token-by-token) ###\n",
    "max_tokens = 10  # Limit the number of generated tokens\n",
    "generated_ids = input_ids\n",
    "\n",
    "print(\"\\nStep-by-Step Generation:\")\n",
    "for step in range(max_tokens):\n",
    "    outputs = model(generated_ids)          # Pass the current tokens into the model\n",
    "    predictions = outputs.logits\n",
    "    \n",
    "    next_token_id = torch.argmax(predictions[:, -1, :], dim=-1)         # Select the most likely next token\n",
    "    \n",
    "    generated_ids = torch.cat((generated_ids, next_token_id.unsqueeze(0)), dim=1)       # Append the next token to the sequence\n",
    "   \n",
    "    current_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)       # Decode and print the current sequence\n",
    "    print(f\"Step {step + 1}: {current_output}\")\n",
    "    print(f\"size of input ndarray is {predictions.size()}\")\n",
    "    \n",
    "    if next_token_id == tokenizer.eos_token_id:             # Stop generation if EOS (end-of-sequence) token is predicted\n",
    "        break\n",
    "\n",
    "print(\"\\nFinal Output:\")\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 50257])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to go deep and explore to see if an LLM like GPT2 architecture is decoder only or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Shape of attention matrix (layer 0): torch.Size([1, 12, 7, 7])\n",
      "Causal mask (first layer, first head):\n",
      "tensor([[1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Encode a sample input\n",
    "input_text = \"What is the capital of France?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the attention mask from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    attentions = outputs.attentions  # List of attention tensors from each layer\n",
    "\n",
    "# Check the shape and behavior of the attention mask\n",
    "print(f\"Number of layers: {len(attentions)}\")\n",
    "print(f\"Shape of attention matrix (layer 0): {attentions[0].shape}\")  # (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "# Verify causal masking in layer 0\n",
    "causal_mask = attentions[0][0][0]  # Extract attention matrix for first head in first layer\n",
    "print(\"Causal mask (first layer, first head):\")\n",
    "print((causal_mask > 0).int())  # Display binary causal mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt: 'What is the capital of France?'\n",
      "Number of tokens: 7\n",
      "Shape of hidden states (last layer): torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define input prompt\n",
    "input_prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass to get hidden states (contextualized embeddings)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states  # Hidden states for all layers\n",
    "\n",
    "# Display shape of hidden states\n",
    "print(f\"Input prompt: '{input_prompt}'\")\n",
    "print(f\"Number of tokens: {len(inputs['input_ids'][0])}\")\n",
    "print(f\"Shape of hidden states (last layer): {hidden_states[-1].shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
