<div align="center">
  <h1>ğŸ—£Let's Think out Load: Large Language Model exploration</h1>
<!--   <p align="center">
    ğŸ¦ <a href="https://twitter.com/maximelabonne">Follow me on X</a> â€¢ 
    ğŸ¤— <a href="https://huggingface.co/mlabonne">Hugging Face</a> â€¢ 
    ğŸ’» <a href="https://mlabonne.github.io/blog">Blog</a> â€¢ 
    ğŸ“™ <a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python">Hands-on GNN</a>
  </p> -->
</div>
<br/>

In this exploration, we aim to delve deeply into the details and concepts within the realm of Large Language Models (LLMs). This journey is not about having all the answers upfront but about thinking out loud, openly sharing ideas, revisiting thoughts, and refining our understanding as we progress. Mistakes and course corrections are not just expected but embraced as part of the process.

This repository is a collaborative space where every comment, suggestion, or insight is valuable and can lead to meaningful discoveries. Together, letâ€™s embark on this exciting adventure into the world of LLMs and uncover their incredible potential.

Letâ€™s get started, 

## How Does LLM Genrates answer?
LLMs leverage the autoregressive method, where the next token in a sequence is predicted based on all previously generated tokens. This approach enables LLMs to generate text step-by-step, with each token conditioned on the context of prior tokens, allowing coherent and contextually relevant outputs. Actually the words comes from:
